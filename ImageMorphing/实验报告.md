# 实验报告 

【注】代码运行说明见根目录下 `ReadMe.md`；首先将所有`source\target`对resize到相同大小。最终结果在`finalres`目录下有副本。

# `Image Fusion`

- `code`文件：

  - `dot_mask.py[预处理：提取source中的边界点与其余内部点，将原图与mask点乘]`
  -  `fusion.py[核心：泊松算法求解fusion图内部点的RGB]`

- #### 算法思路

  - ##### 核心算法 —— `Poisson Image Editing`

    ###### 目标问题：

    【`g为source中的颜色，f为融合后区域内部点的颜色，f*为target图的颜色`】

    <img src="C:\Users\L\AppData\Roaming\Typora\typora-user-images\image-20210421143812234.png" alt="image-20210421143812234" style="zoom:30%;" /><img src="C:\Users\L\AppData\Roaming\Typora\typora-user-images\image-20210421143740628.png" alt="image-20210421143740628" style="zoom:30%; margin-left:10%" />

    ###### 问题转化及解决

    $$
    min_f\int\int_\Omega {|\nabla f - \nabla g|}^2\quad \Leftrightarrow \quad min_f(\Delta f=\Delta g) \\
    其中，\nabla f、\Delta f分别表示像素点的梯度和散度
    $$

    $$
    由一维二阶微分公式 \frac{d^2f(x)}{d^2x}=\lim_{h\rightarrow 0}\frac{f(x+h)-2f(x)+f(x-h)}{2h^2}, \\升维后 \Delta = \frac{\partial}{\partial x^2} + \frac{\partial}{\partial y^2}可得到卷积核：\\
      cov = \left[{\begin{matrix}
       0 & 1 & 0 \\
       0 & -2 & 0 \\
       0 & 1 & 0
      \end{matrix}}\right] +  \left[{\begin{matrix}
       0 & 0 & 0 \\
       1 & -2 & 1 \\
       0 & 0 & 0
      \end{matrix}}\right]= \left[{\begin{matrix}
       0 & 1 & 0 \\
       1 & -4 & 1 \\
       0 & 1 & 0
      \end{matrix}}\right]\\
      因此，【将conv与source卷积，得到原图【标注区域】内各点的散度\Delta g】。对于融合后得图，\Omega/\partial \Omega中点的颜色为待求未知量。\\
      对\Omega内所有进行编号，得到邻接矩阵A、解条件b.其中编号i的点满足：\\
      \begin{align}
      &（1）若i \in \partial \Omega,则A_{ii} = 1, A_{ij} = 0(j \neq i), b[j]为j在target中的颜色；\\
      &（2）否则，A_{ii}=-4，如果j \neq i在source中是i的邻居，则A_{ij} = 1,b[j]=\Delta g[j].\\
      \end{align} \\
      方程Ax = b（逐颜色通道进行求解）的解即为\Omega/\partial \Omega点的最终颜色。
    $$

    ###### 核心代码

    ```python
    def fusion(pid, post, offset):
        '''略去IO、上述变量初始化'''
    
        # edge -- keep the same
        for pt_id in range(edge_size):
            id_in_matrix = pt_id + inner_size
            A[id_in_matrix][id_in_matrix] = 1
            [x, y] = edge[pt_id]
            b[id_in_matrix][:] = origin_img[x + offset[0]][y + offset[1]][:]
    
        # inner points: neighbors; b = original divergence
        for pt_id in range(inner_size):
            A[pt_id][pt_id] = - 4
            pt = inner[pt_id]
    
            left = [pt[0] - 1, pt[1]]
            right = [pt[0] + 1, pt[1]]
            up = [pt[0], pt[1] - 1]
            down = [pt[0], pt[1] + 1]
            neighbor = [left, right, up, down]
    
            for nb in neighbor:
                try:
                    index = inner.index(nb)
                    A[pt_id][index] = 1
                except:
                    try:
                        index = edge.index(nb)
                        A[pt_id][index + inner_size] = 1
                    except:
                        pass
    
            b[pt_id][:] = original_grads[pt_id][:]
    
        # solve the equation
        A = np.mat(A)
        x = np.zeros((inner_size + edge_size, 3))
        for color_channel in range(3):
            bb = b[:, color_channel]
            xx = np.linalg.solve(A, bb)
            x[:, color_channel] = xx
    
        max_color = np.max(x)
    
        for pt in edge:
            brutal_img[pt[0] + offset[0]][pt[1] + offset[1]][:] = origin_img[pt[0] + offset[0]][pt[1] + offset[1]][:]
        for pt_id in range(inner_size):
            pt = inner[pt_id]
            brutal_img[pt[0] + offset[0]][pt[1] + offset[1]][:] = np.array(x[pt_id][:])
    ```

    

  - ##### 预处理方法

    - 将source与mask对应点的像素值相乘并除以255; 

    - 人工选择区域在target中的插入位点，并对上述乘积图、mask进行裁剪；

    - 用裁剪后的mask计算边界点 `edge`、真内部点`inner`的像素坐标：
      $$
      \begin{align}
      & inner = \Omega/\partial \Omega： 【g(p) \neq [0,0,0]】 \and 【g(p's\quad any \quad neighbor)\neq [0,0,0]】\\
      & edge =\partial \Omega： 【g(p) \neq [0,0,0]】 \and 【g(p's\quad any \quad neighbor) = [0,0,0]】
      \end{align}
      $$

    - 将相关数据带入【核心算法】计算可得到最终结果。

- ### 实验结果

  【注】左侧为采用`Poisson Image Editing`算法的结果，右侧为暴力地直接将source塞入target的结果。

  - `test1`

    <img src="E:\LearningData\LearningData\ImageMorphing\image fusion\fusion_1.jpg" alt="fusion_1" style="zoom: 28%;"/><img src="E:\LearningData\LearningData\ImageMorphing\image fusion\test1_target_brutal.jpg" alt="test1_target_brutal" style="zoom: 28%;margin-left:5%" />

    ---

  - `test2`

    <img src="E:\LearningData\LearningData\ImageMorphing\image fusion\fusion_2.jpg" alt="fusion_2" style="zoom:40%;padding-left:5%"/><img src="E:\LearningData\LearningData\ImageMorphing\image fusion\test2_target_brutal.png" alt="test2_target_brutal" style="zoom:40%;margin-left:25%" />

    对于该图片，若将融合图的散度设置为source图散度的**相反数**，则会得到如下结果：

    <img src="E:\LearningData\LearningData\ImageMorphing\image fusion\fusion_2_beutiful.jpg" alt="fusion_2_beutiful" style="zoom:50%;" />



---

# `Face Morphing`

- `code文件`：

  - `hand_label.py：图片特征点标注；`
  - `PointsExtractor.py：将手动标注的点与dlib标注的点融合【除狮子外，其余图像除边框8点外均完全由dlib自动标注】`
  - `triangle.py：读取特征点json文件并格式化+计算给定点的Delaunay剖分`
  - `morphing.py:【核心算法】image morphing算法的核心实现`

- #### 算法思路

  ##### 核心算法

  - ###### 特征点提取

    手动标注（狮子全标，其余标注图片四顶点、各边中点）+ `dlib`库函数`shape_predictor（其预训练模型在model文件夹下）`

  - ###### Morphing

    $$
    \begin{align}
    & (1)由参数t计算source、target特征点的加权平均值:\\
    & \qquad middle\_points = （1-t）*src\_feature\_points + t*trg\_feature\_points;\\
    & (2)用middle\_points进行Delaunay三角剖分（scipy库函数）;\\
    & (3)用剖分返回的索引数组分别计算source、target、middle的三角网格；\\
    & (4)分别计算middle到source、target各三角片的affine\quad matrix(inverse\quad warping),存入矩阵数组;\\
    & (5)对于合成图的像素点(i,j)，用Delaunay对象的find\_simplex函数找到其所属三角片的索引（与矩阵索引相同），\\
    & 分别计算其在source、target中对应点坐标，四点interpolation得到color\_s,color\_t, (i,j)点最终的颜色为：\\
    & \qquad \qquad \qquad \qquad color\_final = (1-t)*color\_s + t*color\_t \\
    & 【注：affine\quad matrix的求解方法在小作业2中已详述】
    \end{align}
    $$

  - 

  - ###### 核心代码

    ```python
    def main(t, source_mesh, target_mesh, src_png, trg_png, pid=1, base_dir=r'./face morphing/', middle_write=True):
        # img label array: 'x' is **vertical**!
        source = np.array(cv2.imread(base_dir + src_png), dtype='int32')
        target = np.array(cv2.imread(base_dir + trg_png), dtype='int32')
    
        # label for draw mesh lines
        source_for_mesh = source.copy()
        target_for_mesh = target.copy()
    
        s_shape = source.shape
        t_shape = target.shape
        shape_0 = min(s_shape[0], t_shape[0])
        shape_1 = min(s_shape[1], t_shape[1])
    
        # delaunay is computed by weighted mixture
        delaunay_0 = from_img_point_to_mesh(source_mesh, target_mesh, t)
        delaunay = delaunay_0.simplices.copy()
    
        matrix_group_m_s = []
        matrix_group_m_t = []
    
        # calculate all affine matrix for each triangle at once
        for i in range(delaunay.shape[0]):
            delta_index = delaunay[i]
            one = delta_index[0]
            two = delta_index[1]
            thr = delta_index[2]
    
            # Find Triangle Vertex, 'x' is horizontal
            source_vertex = np.array([source_mesh[one], source_mesh[two], source_mesh[thr]])
            target_vertex = np.array([target_mesh[one], target_mesh[two], target_mesh[thr]])
    
            # Draw mesh lines and note the triangle index__just for process debug
            cv2.polylines(source_for_mesh, [source_vertex], True, (0, 0, 255))
            cv2.polylines(target_for_mesh, [target_vertex], True, (0, 0, 255))
            s_center = center(source_vertex)
            t_center = center(target_vertex)
            cv2.putText(source_for_mesh, str(i), (s_center[0], s_center[1]), cv2.FONT_HERSHEY_COMPLEX, 0.3, (255, 0, 0), 0)
            cv2.putText(target_for_mesh, str(i), (t_center[0], t_center[1]), cv2.FONT_HERSHEY_COMPLEX, 0.3, (255, 0, 0), 0)
    
            # get affine matrix respectively -- inverse warping
            middle_vertex = (1-t) * source_vertex + t * target_vertex
            middle_vertex = middle_vertex.astype(int)
    
            matrix_s_m = cv2.getAffineTransform(np.float32(middle_vertex), np.float32(source_vertex))
            matrix_group_m_s.append(matrix_s_m)
    
            matrix_t_m = cv2.getAffineTransform(np.float32(middle_vertex), np.float32(target_vertex))
            matrix_group_m_t.append(matrix_t_m)
    
        print('=====================MATRIX CALC DONE======================')
    
        # TODO: label for affine warping
        middle_label_s = source.copy()
        middle_label_t = target.copy()
        final = np.random.randn(shape_0, shape_1, 3)
        # TODO: for each pixel in final label, inverse warping__find color in s and t
        for i in range(shape_0):
            for j in range(shape_1):
                mesh_id = delaunay_0.find_simplex([j, i])
                matrix_s_m = matrix_group_m_s[mesh_id]
                matrix_t_m = matrix_group_m_t[mesh_id]
    
                # TODO: when get matrix, the order is (shape[1], shape[0])
                color_s = get_pixel_color([j, i], matrix_s_m, source)
                color_t = get_pixel_color([j, i], matrix_t_m, target)
    
                middle_label_s[i][j][:] = color_s[:]
                middle_label_t[i][j][:] = color_t[:]
                for color_id in range(3):
                    final[i][j][color_id] = t * color_t[color_id] + (1-t) * color_s[color_id]
    ```

    

  ##### 实现遇到主要问题及解决

  - ###### 数组维数顺序

    `dlib`库、手工标注点坐标[x,y]中，x代表横向，y代表纵向，这与`cv2.imread`得到的`numpy.array`恰恰相反，所以在 `get_pixel_color()`函数中`点参数`传入需要注意维度顺序，否则得到以下错误结果（t=0.5）：

    <img src="C:\Users\L\AppData\Roaming\Typora\typora-user-images\image-20210421164305769.png" alt="image-20210421164305769" style="zoom:50%;" />

  - ###### 内部点的确定

    即在剖分、得到三角片后，如何将图片中的各像素点归到各自的三角片内。最初计算混合图时，我遍历各三角片，求其包围盒，并判断包围盒中哪些点在三角形内部，将这些点一起warp。由于数据精度的舍入误差，这样会造成某些点被重复包含在不同三角片中，以至于warping后图像会产生较大的畸变。后改为遍历所有点，找其所在三角片索引的方法。以下为`t=0.8时，test2`使用前者warp的效果：

    【左侧为source warp后的效果；右图为最终得到的结果，可以看到狮子嘴部、眼部有明显的三角片轮廓】

    <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\source_2_m_t_0.25.jpg" alt="source_2_m_t_0.25" style="zoom:50%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.25.jpg" alt="2_final_t_0.25" style="zoom:50%; margin-left:15%" />

    后者相同参数下实现效果：

    <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\source_2_m_t_0.8.jpg" alt="source_2_m_t_0.8" style="zoom:50%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\target_2_m_t_0.8.jpg" alt="target_2_m_t_0.8" style="zoom:50%; margin-left:15%" />

- #### 实验结果

  ###### 【从左到右，t=0.1~0.9】

  ###### `test1`

  # <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.1.jpg" alt="1_final_t_0.1" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.2.jpg" alt="1_final_t_0.2" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.3.jpg" alt="1_final_t_0.3" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.4.jpg" alt="1_final_t_0.4" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.5.jpg" alt="1_final_t_0.5" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.6.jpg" alt="1_final_t_0.6" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.7.jpg" alt="1_final_t_0.7" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.8.jpg" alt="1_final_t_0.8" style="zoom: 17%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.9.jpg" alt="1_final_t_0.9" style="zoom: 17%;" />

  ###### `test2`

  # <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.1.jpg" alt="2_final_t_0.1" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.2.jpg" alt="2_final_t_0.2" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.3.jpg" alt="2_final_t_0.3" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.4.jpg" alt="2_final_t_0.4" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.5.jpg" alt="2_final_t_0.5" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.6.jpg" alt="2_final_t_0.6" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.7.jpg" alt="2_final_t_0.7" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.8.jpg" alt="2_final_t_0.8" style="zoom: 19%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.9.jpg" alt="2_final_t_0.9" style="zoom: 19%;" />

  ###### `t=0.5时 warp+morph全流程`

  # <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\source1.png" alt="source1" style="zoom:33%;" />	**+**	<img src="E:\LearningData\LearningData\ImageMorphing\face morphing\target1.png" alt="target1" style="zoom:33%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\source2.png" alt="source2" style="zoom:35%; margin-left:10%" />	+	<img src="E:\LearningData\LearningData\ImageMorphing\face morphing\target2.png" alt="target2" style="zoom:35%;" />

  # <img src="C:\Users\L\Pictures\下箭头.png" alt="下箭头" style="zoom:25%; margin-left:3%" /><img src="C:\Users\L\Pictures\下箭头.png" alt="下箭头" style="zoom:25%; margin-left:18%" /><img src="C:\Users\L\Pictures\下箭头.png" alt="下箭头" style="zoom:25%; margin-left:20%" /><img src="C:\Users\L\Pictures\下箭头.png" alt="下箭头" style="zoom:25%; margin-left:14%" />

  # <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\source_1_m_t_0.5.jpg" alt="source_1_m_t_0.5" style="zoom:33%;" />	**+**	<img src="E:\LearningData\LearningData\ImageMorphing\face morphing\target_1_m_t_0.5.jpg" alt="target_1_m_t_0.5" style="zoom:33%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\source_2_m_t_0.5.jpg" alt="source_2_m_t_0.5" style="zoom:35%; margin-left:10%" />	+	<img src="E:\LearningData\LearningData\ImageMorphing\face morphing\target_2_m_t_0.5.jpg" alt="target_2_m_t_0.5" style="zoom:35%;" />	

  # <img src="C:\Users\L\Pictures\下箭头.png" alt="下箭头" style="zoom:25%; margin-left:17%" /><img src="C:\Users\L\Pictures\下箭头.png" alt="下箭头" style="zoom:25%; margin-left:44%" />

  # <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\1_final_t_0.5.jpg" alt="1_final_t_0.5" style="zoom:33%;padding-left:13%" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\2_final_t_0.5.jpg" alt="2_final_t_0.5" style="zoom: 35%;margin-left:34%" />

  ###### 特征点三角网格图示（为方便调试，将三角片索引也标注出）

  <img src="E:\LearningData\LearningData\ImageMorphing\face morphing\source_1_mesh_t_0.5.jpg" alt="source_1_mesh_t_0.5" style="zoom: 50%;" /><img src="E:\LearningData\LearningData\ImageMorphing\face morphing\target_1_mesh_t_0.5.jpg" alt="target_1_mesh_t_0.5" style="zoom: 50%;margin-left:15%" />



# `View Morphing`

- `code`文件

  - `face morphing`中所有文件
  - `view_morphing.py:【视图变换核心算法】`

- ### 算法思路

  完全按照参考文献[[1\]](#_ftn1)实现：
  $$
  \begin{align}
  &(1)对source、target进行特征点提取后，用文献附录中的方法计算旋转矩阵H0、H1（其中，I_0对应R_d^{\theta}的旋转轴取与极点e_0垂直的点）,使得对应特征点的y值（纵向值）再同一条扫描线上;\\
  &(2)对旋转后的I_0^{'}、I_1^{'}进行morph（调用face morphing中的morph函数）;\\
  &(3)混合后的图片显示在弹框中，手动标注四个顶角（顺序：左上，左下，右上，右下），计算H_s得到最终图像。
  
  \end{align}
  $$

  ###### 核心代码

  ```python
  def view_morphing(pid):
      H0, H1, src_points, trg_points = pre_warping(pid)
      src_points_list = []
      trg_points_list = []
      for i in range(src_points.shape[0]):
          src_points_list.append([src_points[i][0], src_points[i][1]])
          trg_points_list.append([trg_points[i][0], trg_points[i][1]])
      # print(H0， H1)
  
      source = np.array(cv2.imread(r'./view morphing/source_' + str(pid) + '.png'))
      target = np.array(cv2.imread(r'./view morphing/target_' + str(pid) + '.png'))
      s_feature_after = []
      t_feature_after = []
      if pid == 2:
          source = swap_x_y(source)
          target = swap_x_y(target)
  
      s0 = source.shape[0]
      s1 = source.shape[1]
  
      extend = int(math.sqrt(source.shape[0] ** 2 + source.shape[1] ** 2))
      if pid == 1:
          s_pre = cv2.warpPerspective(source / 255, H0, (s1, s0))
          t_pre = cv2.warpPerspective(target / 255, H1, (s1, s0))
      else:
          # 针对像素丢失的现象做处理，详见view_morphing.py源文件
  
          # TODO: merge the warped legal feature points
          for f_id in range(len(s_f_id)):
              try:
                  t_id = t_f_id.index(s_f_id[f_id])
                  s_feature_after.append(s_f_tmp[f_id])
                  # cv2.circle(s_pre, (s_f_tmp[f_id][0], s_f_tmp[f_id][1]), 2, (0, 255, 0), 1)
                  t_feature_after.append(t_f_tmp[t_id])
                  # cv2.circle(t_pre, (t_f_tmp[t_id][0], t_f_tmp[t_id][1]), 2, (0, 255, 0), 1)
              except:
                  pass
  
      cv2.imwrite(r'./view morphing/s_' + str(pid) + '_pre.png', (np.clip(s_pre, 0., 1.) * 255).astype(np.uint8))
      cv2.imwrite(r'./view morphing/t_' + str(pid) + '_pre.png', (np.clip(t_pre, 0., 1.) * 255).astype(np.uint8))
  
      # TODO: calc position for feature points after rotate
      # TODO: for test 2, features also needs adjust
      if pid == 1:
          for pt in src_points:
              pt_after = np.dot(H0, np.array([pt[0], pt[1], 1]))
              s_feature_after.append([pt_after[0] / pt_after[2], pt_after[1] / pt_after[2]])
  
          for pt in trg_points:
              pt_after = np.dot(H1, np.array([pt[0], pt[1], 1]))
              t_feature_after.append([pt_after[0] / pt_after[2], pt_after[1] / pt_after[2]])
  
      json.dump({'data': s_feature_after}, open(r'./view morphing/s_' + str(pid) + '_pre.png_array.json', 'w'))
      json.dump({'data': t_feature_after}, open(r'./view morphing/t_' + str(pid) + '_pre.png_array.json', 'w'))
  
      s_feature_after = np.array(s_feature_after, dtype='int32')
      t_feature_after = np.array(t_feature_after, dtype='int32')
  
  
      # TODO: morph
      for i in range(1, 10):
          t = float('%0.1f' % (i * 0.1))
          main(t, s_feature_after, t_feature_after, 's_' + str(pid) + '_pre.png', 't_' + str(pid) + '_pre.png', pid=pid,
               base_dir='./view morphing/', middle_write=True)
          final_b = np.array(cv2.imread(r'./view morphing/' + str(pid) + '_final_t_' + str(t) + '.jpg'), dtype='int32')
          final_b = final_b / 255
          # TODO: manually select the matrix Hs
          a, b = label_one_img(r'./view morphing/' + str(pid) + '_final_t_' + str(t) + '.jpg')
          source_points = np.array([[a[k], b[k]] for k in range(4)]).astype('float32')
          target_points = np.array([[0, 0], [0, s0], [s1, 0], [s1, s0]]).astype('float32')
          print(source_points)
          print(target_points)
          Hs = cv2.getPerspectiveTransform(source_points, target_points)
  
          final = cv2.warpPerspective(final_b, Hs, (s1, s0), borderMode=cv2.BORDER_REPLICATE)
          if pid == 2:
              final = swap_x_y(final)
          cv2.imwrite(r'./view morphing/warped_' + str(pid) + '_' + str(t) + '.png',
                      (np.clip(final, 0., 1.) * 255).astype(np.uint8))
  ```

  

- ### 实验结果

  【注：为保证`prewarp`代码的通用性，将`test2`先逆时针旋转90°，最终再变回来；为解决`prewarp`中source经过`H0`变换后得到负数坐标/坐标超过原图像大小，将生成图像长、宽设为变换后对应维度绝对值最大元素的两倍，否则会丢失像素】

  ###### `test1`

  ###### `prewarp`

  <img src="E:\LearningData\LearningData\ImageMorphing\view morphing\s_1_pre.png" alt="s_1_pre" style="zoom:50%;padding-left:10%" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\t_1_pre.png" alt="t_1_pre" style="zoom:50%;margin-left:10%" />

  ###### `final results`

  <img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.1.png" alt="warped_1_0.1" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.2.png" alt="warped_1_0.2" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.3.png" alt="warped_1_0.3" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.4.png" alt="warped_1_0.4" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.5.png" alt="warped_1_0.5" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.6.png" alt="warped_1_0.6" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.7.png" alt="warped_1_0.7" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.8.png" alt="warped_1_0.8" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_1_0.9.png" alt="warped_1_0.9" style="zoom:18%;" />

  ###### `test2`

  ###### `prewarp`

  <img src="E:\LearningData\LearningData\ImageMorphing\view morphing\s_2_pre.png" alt="s_2_pre" style="zoom:33%;" />

  <img src="E:\LearningData\LearningData\ImageMorphing\view morphing\t_2_pre.png" alt="t_2_pre" style="zoom:33%;" />

  `final results`

  <img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.1.png" alt="warped_2_0.1" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.2.png" alt="warped_2_0.2" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.3.png" alt="warped_2_0.3" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.4.png" alt="warped_2_0.4" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.5.png" alt="warped_2_0.5" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.6.png" alt="warped_2_0.6" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.7.png" alt="warped_2_0.7" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.8.png" alt="warped_2_0.8" style="zoom:18%;" /><img src="E:\LearningData\LearningData\ImageMorphing\view morphing\warped_2_0.9.png" alt="warped_2_0.9" style="zoom:18%;" />













---

[[1\]](#_ftnref1)Steven MSeitz and Charles RDyer.View morphing . pages 21–30,1996.